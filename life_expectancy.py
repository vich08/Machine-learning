# -*- coding: utf-8 -*-
"""Life_expectancy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19aAe4qH4oUrKGupo13ciXFG6dZ7eDDB0

# Introducción
Se adjuntan a continuación los datos para la PEP 2

En este caso se ocupará el metodo de regresión lineal


Se realizaron las siguientes acciones para trabajar el data base

*  Cargar los datos de training y botar los valores nulo

*   Entrenar modelos con estos datos limpiados a la "rápida" y evaluar desempeño

*  Con el dataset de testeo usando los alguna de las funciones que trae scikit learn mean_squeare_error, etc.
*  Entrenar modelos con los datos después del procesamiento del EDA, limpieza de datos, nuevas variables, etc.


*  Calcular el error con respecto a los datos de testeo con el nuevo modelo


*   Comparar si hubo una mejora entre los modelos.
"""

import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error , r2_score

df = pd.read_csv('Life Expectancy Data.csv')
df.head()

df.shape

df.describe

plt.figure(figsize=(15, 10))
sns.heatmap(df.corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Mapa de Calor de Correlación")
plt.show()

data_cleaned = df.dropna()

data_cleaned.shape

X_cleaned = data_cleaned.drop(['Life expectancy ','Status','Country'], axis=1)
y_cleaned = data_cleaned['Life expectancy ']

num_vars_cleaned = X_cleaned.select_dtypes(include=['float64', 'int64']).columns
cat_vars_cleaned = X_cleaned.select_dtypes(include=['object']).columns

num_vars_cleaned = X_cleaned.select_dtypes(include=['float64', 'int64']).columns
cat_vars_cleaned = X_cleaned.select_dtypes(include=['object']).columns

num_pipeline_cleaned = Pipeline([
    ('std_scaler', StandardScaler())
])

cat_pipeline_cleaned = Pipeline([
    ('onehot', OneHotEncoder())
])

full_pipeline_cleaned = ColumnTransformer([
    ('num', num_pipeline_cleaned, num_vars_cleaned),
    ('cat', cat_pipeline_cleaned, cat_vars_cleaned)
])

X_prepared_cleaned = full_pipeline_cleaned.fit_transform(X_cleaned)

X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_prepared_cleaned, y_cleaned, test_size=0.2, random_state=42)

X_train_cleaned.shape, X_test_cleaned.shape

linear_model = LinearRegression()
linear_model.fit(X_train_cleaned, y_train_cleaned)

y_pred_linear = linear_model.predict(X_test_cleaned)

mse_linear = mean_squared_error(y_test_cleaned, y_pred_linear)
rmse_linear = np.sqrt(mse_linear)
r2_linear = r2_score(y_test_cleaned, y_pred_linear)

mse_linear, rmse_linear, r2_linear

from sklearn.model_selection import train_test_split
X_train_cleaned, X_test_cleaned, y_train_cleaned, y_test_cleaned = train_test_split(X_cleaned, y_cleaned)

"""Mean absolut error"""

from xgboost import XGBRegressor
model = XGBRegressor()
model.fit(X_train_cleaned,y_train_cleaned)
from sklearn.metrics import mean_absolute_error
predictions = model.predict(X_test_cleaned)
print("MAE: ", str(mean_absolute_error(predictions, y_test_cleaned)))

"""# Random Forest Regressor

"""





"""# Conclusión

En primera instacia se realizó una revisión exhaustiva de los datos, realizando un mapa de calor de correlación de estos, para verificar su calidad y  así proceder a definir nuestra manera de actuar para crear la variable de limpieza y que la base quedara impecable.

Hemos preparado los datos para el modelo de la siguiente manera:


*   Separación de Características y Variable Objetivo: La variable objetivo (Life expectancy) se ha separado de las características.

*   Identificación de Variables Numéricas y Categóricas: Se identificaron variables numéricas y categóricas para su procesamiento por separado.

Pipelines de Preprocesamiento:

*  Para las variables numéricas, se utilizó un input para rellenar valores faltantes con la mediana y un escalador estándar para normalizar los datos.

*  
Para las variables categóricas, se utilizó un input para rellenar valores faltantes con el valor más frecuente y un codificador one-hot para transformarlas en un formato adecuado para el modelo.

*   Combinación de Pipelines: Se combinaron los pipelines en un transformador de columnas.


*   Preprocesamiento de los Datos: Se aplicaron los pipelines al conjunto de datos.

*   División en Conjuntos de Entrenamiento y Prueba: Se dividió el conjunto de datos en un 80% para entrenamiento y un 20% para prueba.


*   El data set tiene 2350 muestras y el conjunto de prueba 588 muestras. Cada muestra tiene 214 características después de la transformación.


Al realizar el analisis EDA con Regresión lineal obtuvimos un MAE de 13.071552546181286 con regresión linear y con XGBoost 1.1246109008789065 demostrando que XGBoost es mucho más efectivo de realizar

Para finalizar podemos decir que al Realizar los 3 análisis de datos y obtener el MAE, método de Regresión linear no nos genera un un análisis de datos efectivo en comparación al XGBoost que implementamos, el cual es importante realizarlo por que la cantidad de datos es pequeña, por lo tanto debemos entregar un valor más preciso con la menor cantidad de perdida de datos, pues un contra menor la cantidad de datos la perdida de estos se vuelve más significativo para el análisis más eficaz.
"""